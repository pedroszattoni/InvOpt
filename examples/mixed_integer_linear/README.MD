
# `mixed_integer_linear`

>Warning: the `mixed_integer_linear` function requires `gurobipy`.

## Use case

This function can be used when the FOP is of the form

$$
\min_ {x \in \mathbb{X}(\hat{s})} \ \langle \theta,\phi(\hat{s}, x) \rangle
$$

with a mixed-integer constraint set

$$
\mathbb{X}(\hat{s}) = \left\\{ x=(y,z) \in \mathbb{R}^u \times \mathbb{Z}^v : \hat{A}y + \hat{B}z \leq \hat{c}, \ z \in \mathbb{Z}(\hat{w}) \right\\},
$$

where the signal in the form $\hat{s} = (\hat{A}, \hat{B}, \hat{c}, \hat{w})$ and $\mathbb{Z}(\hat{w}) \subseteq \mathbb{Z}^v$ is a bounded set that may depend on a signal $\hat{w}$, and the hypothesis function is in the form

$$
\langle \theta,\phi(\hat{s}, x) \rangle = \langle y,Q \phi_ 1(\hat{w}, z) \rangle  + \langle q,\phi_ 2(\hat{w}, z)  \rangle ,
$$

where $x = (y,z)$, $Q \in \mathbb{R}^{u \times m}$, $q \in \mathbb{R}^r$, $\phi_ 1: \mathbb{W} \times \mathbb{Z}^v \to \mathbb{R}^m$ and $\phi_ 2: \mathbb{W} \times \mathbb{Z}^v \to \mathbb{R}^r$ are feature mappings, $\theta = [\text{vec}(Q)  \ \ q] \in \mathbb{R}^p$ and $\phi(\hat{s}, x) = [\phi_ 1(\hat{w}, z) \otimes y \quad \phi_ 2(\hat{w}, z)] \in \mathbb{R}^p$, where $\otimes$ is the Kronecker product.

## Solution method

Given a dataset of signal-response data $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$, we pose the IO problem as a regularized loss minimization problem using the *Augmented Suboptimality loss*:

$$
\min_ {\theta \in \Theta} \ \kappa\mathcal{R}(\theta - \hat{\theta}) + \frac{1}{N}\sum_{i=1}^N \max_ {x_ i \in \mathbb{X}(\hat{s}_ i)} \\{ \langle \theta, \phi(\hat{s}_ i,\hat{x}_ i) - \phi(\hat{s}_ i,x_ i) \rangle + d(\hat{x}_ i,x_ i) \\},
$$

where $\Theta$ is an (optional) set used to encode any prior information or assumption we may have on the expert's true cost function (e.g., nonnegativity), $\kappa \geq 0$ is the regularization parameter, $\mathcal{R} : \mathbb{R}^p \to \mathbb{R}$ is a regularization function, $\hat{\theta} \in \mathbb{R}^p$ is an (optional) a priory belief or estimate of the true cost vector.

We propose to solve this minimax problem by reformulating it as a minimization problem. To do so, we use the distance penalization $d(\hat{x}, x) = \rVert \hat{y} - y \rVert_ \infty + d_ z(\hat{z}, z)$, we define $M_ i = |\mathbb{Z}(\hat{w}_ i)|$ and use the following indexing notation for the elements of $\mathbb{Z}(\hat{w}_ i) = \\{z_ {i1},\ldots,z_ {ij},\ldots,z_ {iM_ i}\\}$, and also define $h_ k \in \mathbb{R}^u$ as the vector of zeros except for the $k$'th element, which equals $1$. If $k > u$, the $(k-u)$'th element equals to $-1$. In this case, we can reformulate the regularized loss minimization problem as

$$
\begin{aligned}
\min \quad & \kappa\mathcal{R}(\theta - \hat{\theta}) + \frac{1}{N}\sum_ {i=1}^N \beta_ i  \\
\text{s.t.} \quad & \theta = (\text{vec}(Q), q) \in \Theta, \quad \lambda_{ijk}\geq 0, \quad \beta_i \in \mathbb{R} & \forall (i, j, k) \in [N] \times [M_ i] \times [2u] \\
& \langle \theta,\phi(\hat{s}_ i,\hat{x}_ i) \rangle + \langle \lambda_ {ijk},\hat{c}_ i - \hat{B}_ i z_ {ij} \rangle - \langle q,\phi_ 2(\hat{w}_ i, z_ {ij}) \rangle  + \langle h_ k,\hat{y}_ i \rangle  + d_ z(\hat{z}_ i, z_ {ij}) \leq \beta_ i & \forall (i, j, k) \in [N] \times [M_ i] \times [2u] \\
& Q \phi_ 1(\hat{w}_ i, z_ {ij}) + h_ k + \hat{A}_ i^\top \lambda_ {ijk} = 0 & \forall (i, j, k) \in [N] \times [M_ i] \times [2u].
\end{aligned}
$$

For more details on this reformulation, see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/2305.07730). Alternatively, to decrease the number of constraints of the reformulation by a factor of $2u$, one can use $d(\hat{x}, x) = d_ z(\hat{z}, z)$, i.e., only penalize the integer part of the decision vector, which is equivalent to setting $h_k =0 \ \ \forall k \in [2u]$. Moreover, the distance function can also take the signal $\hat{w}$ as a third argument. This is useful if one wants the distance to be computed in feature space, that is, $d_ z(\hat{z}, z, \hat{w}) = d_ z(\phi_ 2(\hat{w}, \hat{z}), \phi_ 2(\hat{w}, z))$, which in some cases can improve the performance of the model. If a distance penalization function $d$ is not provided at all, the Augmented Suboptimality loss reduces to the *Suboptimality loss*. In this case, if $\kappa=0$ or $\hat{\theta}$ is not provided, we add a norm equality constraint $\rVert \theta \rVert = 1$ to the reformulation above, to exclude the trivial solution $\theta = 0$. The norm equality constraint uses the $\ell_ 1$ norm if $\Theta = \\{\theta \in \mathbb{R}^p : \theta \geq 0\\}$, and the $\ell_ \infty$ norm otherwise. 

## Example: binary linear mixed-integer program

In the file `MILP.py`, you will find an example usage of the `mixed_integer_linear` function. For this example, the dataset $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$ is generated by solving the binary linear program

$$
\begin{aligned}
\hat{x}_ i = (\hat{y}_ i, \hat{z}_ i) \in \arg\min_ {y, z} \quad &  \langle y,q_ y \rangle + \langle q_z,z \rangle \\
\text{s.t.} \quad & \hat{A}_ i y + \hat{B}_ i z \leq \hat{c}_ i \\
& 0 \leq y \leq 1, \quad z \in \\{0,1\\}^v,
\end{aligned}
$$

where $\hat{s}_ i = ([\hat{A}_ i^\top \ \ I \ \ -I]^\top, [\hat{B}_ i^\top \ \ 0_ {u \times 2u} ]^\top, [\hat{c}_ i \ \ \mathbf{1} \ \ \mathbf{0}], 0)$, where $\mathbf{1}$ is the vector of ones and $\mathbf{0}$ is the vector of zeros. The IO problem is solved using the solution methods presented above, with $Q=q_y$, $q=q_z$, $\phi_ 1(\hat{w},z) = 1$ and $\phi_ 2(\hat{w},z) = z$. For a detailed description of this example, please see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/2305.07730).