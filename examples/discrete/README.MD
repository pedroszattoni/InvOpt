# `discrete`

>Warning: the `discrete` function requires `gurobipy`.

## Use cases

This function is can be used when the FOP is of the form

$$
\min_ {x \in \mathbb{X}(\hat{s})} \ \theta^\top \phi(\hat{s},x)
$$

and the constraint set $\mathbb{X}(\hat{s})$ is discrete, that is, we can list all elements in this set.

## Solution method

Given the a dataset of signal-response data $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$,  the `discrete` function solves the IO problem using the an epigraph reformulation of the *Augmented Suboptimality loss*,

$$
\begin{aligned}
 \min_ {\theta, \beta_1, \ldots, \beta_N} \quad & \kappa\mathcal{R} (\theta - \hat{\theta}) + \frac{1}{N}\sum_ {i = 1}^N \beta_ i \\ 
 \text{s.t.} \ \ \ \quad & \theta^\top \big(\phi(\hat{s}_ i,\hat{x}_ i) - \phi(\hat{s}_ i,x_ i) \big) + d(\hat{x}_ i,x_ i)\leq \beta_i & \forall x_ i \in \mathbb{X}(\hat{s}_ i), \ \forall i \in [N] \\
 & \theta \in \Theta, \quad \beta_i \geq 0 & \forall i \in [N],
\end{aligned}
$$

where
- $\hat{\theta} \in \mathbb{R}^p$ is an a priory belief or estimate of the true cost vector. *(optional)*
- $\mathcal{R} : \mathbb{R}^p \to \mathbb{R}$ is a regularization function.
- $\kappa$ is a nonnegative regularization parameter.
- $\phi: \mathbb{S} \times \mathbb{X} \to \mathbb{R}^p$ is the feature mapping, which maps a signal-response pair $(s,x)$ to a feature vector $\phi(s,x)$.
- $d : \mathbb{X} \times \mathbb{X} \to \mathbb{R}_+$ is a distance function, which given two responses, returns the distance between them according to some distance metric. *(optional)*
- $\mathbb{X}(\hat{s}_ i)$ is the constraint set of the FOP.
- $\Theta$ is the set used to encode any prior information or assumption we may have on the expert's true cost function, e.g., nonnegativity of the cost vector. *(optional)*

Alternatively, if the distance penalization function $d$  is not provided and $\kappa=0$ or $\hat{\theta}$ is not provided, the `discrete` solves the IO problem using the epigraph reformulation of the *Suboptimality loss*

$$
\begin{aligned} \min_ {\theta, \beta_1, \ldots, \beta_N} \quad & \frac{1}{N}\sum_ {i = 1}^N \beta_i \\
\text{s.t.} \ \ \ \quad & \theta^\top \big(\phi(\hat{s}_ i,\hat{x}_ i) - \phi(\hat{s}_ i,x_ i) \big) \leq \beta_i & \forall x_ i \in \mathbb{X}(\hat{s}_ i), \ \forall i \in [N] \\
& \theta \in \Theta, \quad \rVert \theta \rVert = 1, \quad \beta_i \geq 0 & \forall i \in [N],
\end{aligned}
$$

where the norm equality constraint uses the $\ell_ 1$ norm if $\Theta = \\{\theta \in \mathbb{R}^p : \theta \geq 0\\}$, and the $\ell_ \infty$ norm otherwise. This equality constraint must be added to exclude the trivial solution $\theta = 0$.

## Example: binary linear program with inconsistent data

In the file `binary_LP_inconsistent_data.py`, you will find an example usage of the `discrete` function. For this example, the dataset $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$ is generated by solving the binary linear program

$$
\begin{aligned}
\hat{x}_ i \in \arg\min_ {x} \quad &  (\theta + \sigma_ i)^\top x \\
\text{s.t.} \quad & A_ i x \leq b_ i \\
& x \in \\{0,1\\}^n,
\end{aligned}
$$

where $\hat{s}_ i = (A_ i, b_ i)$, $\sigma_i$ is a Gaussian noise vector, and the IO problem is solved using the Augmented Suboptimality loss and Suboptimality loss reformulations. For a detailed description of this example, please see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/2305.07730).