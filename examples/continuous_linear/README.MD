# `continuous_linear`

>Warning: the `continuous_linear` function requires `gurobipy`.

## Use cases

This function is can be used when the FOP is of the form

$$
\min_ {x \in \mathbb{X}(\hat{s})} \ \langle \theta, \phi(\hat{s}, x)\rangle 
$$

with a continuous constraint set

$$
\mathbb{X}(\hat{s}) = \left\\{ x \in \mathbb{R}^n : \hat{A}x \leq \hat{b} \right\\},
$$

where $\hat{s} = (\hat{A}, \hat{b}, \hat{w})$, and hypothesis function

$$
\langle \theta,\phi(\hat{s}, x) \rangle = \langle x,Q \phi_ 1(\hat{w}) \rangle,
$$

where $\theta = \text{vec}(Q)$, and $\phi_ 1$ is a feature function.

## Solution method

We define $\gamma_ k \in \mathbb{R}^n$ equal the vector of zeros except for the $k$'th element, which equals $1$ if $k \leq n$. If $k > n$, then the $\text{mod}(k,n)$'th element equals $-1$. Given the a dataset of signal-response data $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$,  the `continuous_linear` function solves the IO problem using

$$
\begin{aligned}
\min_{\theta, \beta_ i, \lambda_ {ik}} \quad & \kappa\mathcal{R}(\theta - \hat{\theta}) + \frac{1}{N}\sum_ {i=1}^N \beta_ i  \\
\text{s.t.} \ \quad & \langle \theta,\phi(\hat{s}_ i,\hat{x}_ i) \rangle  + \langle \lambda_ {ik},\hat{c}_ i \rangle + \langle \gamma_ k,\hat{y}_ i \rangle  \leq \beta_ i & \forall i \in [N], \ \forall k \in [2n] \\
& Q \phi_ 1(\hat{w}_ i) + \gamma_ k + \hat{A}_ i^\top \lambda_ {ik} = 0 & \forall i \in [N], \ \forall k \in [2n] \\
& \theta \in \Theta, \quad \beta_i \geq 0, \quad \lambda_ {ik} \geq 0 & \forall i \in [N], \ \forall k \in [2n],
\end{aligned}
$$

where
- $\hat{\theta} \in \mathbb{R}^p$ is an a priory belief or estimate of the true cost vector. *(optional)*
- $\mathcal{R} : \mathbb{R}^p \to \mathbb{R}$ is a regularization function.
- $\kappa$ is a nonnegative regularization parameter.
- $\phi_ 1: \mathbb{W} \to \mathbb{R}^m$ is the feature mapping, which maps $\hat{w}$ to a feature vector $\phi_ 1(\hat{w})$.
- $\Theta$ is the set used to encode any prior information or assumption we may have on the expert's true cost function, e.g., nonnegativity of the cost vector. *(optional)*

Alternatively, if $\kappa=0$ or $\hat{\theta}$ is not provided, the `continuous_linear` function solves the IO problem using the *Suboptimality loss*-based reformulation

$$
\begin{aligned}
\min_{\theta, \beta_ i, \lambda_ {i}} \quad & \frac{1}{N}\sum_ {i=1}^N \beta_ i  \\
\text{s.t.} \ \quad & \langle \theta,\phi(\hat{s}_ i,\hat{x}_ i) \rangle  + \langle \lambda_ {i}, \hat{c}_ i \rangle  \leq \beta_ i & \forall i \in [N] \\
& Q \phi_ 1(\hat{w}_ i) + \hat{A}_ i^\top \lambda_ {i} = 0 & \forall i \in [N] \\
& \theta \in \Theta, \quad \rVert \theta \rVert = 1, \quad \beta_i \geq 0, \quad \lambda_ {i} \geq 0 & \forall i \in [N],
\end{aligned}
$$

where the norm equality constraint uses the $\ell_ 1$ norm if $\Theta = \\{\theta \in \mathbb{R}^p : \theta \geq 0\\}$, and the $\ell_ \infty$ norm otherwise. This equality constraint must be added to exclude the trivial solution $\theta = 0$.

## Example: linear program

In the file `LP.py`, you will find an example usage for the `continuous_linear` function. For this example, the dataset $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$ is generated by solving the linear program

$$
\begin{aligned}
\hat{x}_ i \in \arg\min_ {x} \quad &  \langle x,q \rangle \\
\text{s.t.} \quad & \hat{A}_ i x \leq \hat{b}_ i \\
& 0 \leq x \leq 1,
\end{aligned}
$$

where $\hat{s}_ i = ([\hat{A}_ i^\top \ \ I \ \ -I]^\top, [\hat{b}_ i \ \ \mathbf{1} \ \ \mathbf{0}], 0)$, where $\mathbf{1}$ is the vector of ones and $\mathbf{0}$ is the vector of zeros. The IO problem is solved using the solution methods presented above, with $Q=q$ and $\phi_ 1(\hat{w}) = 1$. The way the synthetic data is generated and the way the results are evaluated follow the same template as the numerical experiments in [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/2305.07730).