# `FOM`

## Use case

This function can be for general FOPs of the form

$$
\min_ {x \in \mathbb{X}(\hat{s})} \ \langle \theta,\phi(\hat{s},x) \rangle .
$$

## Solution method

Given a dataset of signal-response data $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$,  the `FOM` function solves the IO problem by minimizing the empirical risk minimization problem

$$
\min_ {\theta \in \Theta} \ \kappa\mathcal{R}(\theta - \hat{\theta}) + \frac{1}{N}\sum_{i=1}^N \ell_\theta (\hat{s}_i, \hat{x}_i)
$$

using first-order methods, where $\ell_\theta$ is the loss function. When using the Augmented Suboptimality loss, the first-order algorithm uses the augmented FOP oracle

$$
\text{A-FOP}(\theta, \hat{s}, \hat{x}) = \arg\min_ {x \in \mathbb{X}(\hat{s})} \big\\{ \langle \theta,\phi(\hat{s},x) \rangle  - d(\hat{x},x) \big\\}.
$$

When minimizing the Suboptimality loss, the algorithm uses a FOP oracle

$$
\text{FOP}(\theta, \hat{s}) = \arg\min_ {x \in \mathbb{X}(\hat{s})} \langle \theta,\phi(\hat{s},x) \rangle.
$$
___
**First-order algorithm**
> **for** $t=1, \ldots, T$ **do**:  
> $\quad$ Sample a batch of examples  $B \subset \\{1, \ldots, N\\}$  
> $\quad$ Compute $x_ j = \text{A-FOP}(\theta_ t, \hat{s}_ j, \hat{x}_ j)$ or $x_ j = \text{FOP}(\theta_ t, \hat{s}_ j)$ for all $j \in B$  
> $\quad$ Compute $g = \sum_ {j \in B} \left( \kappa \nabla\mathcal{R}(\theta_ t - \hat{\theta}) + \phi(\hat{s}_ j,\hat{x}_ j) - \phi(\hat{s}_ j,x_ j) \right)$  
> $\quad$ Update step: $\theta_ {t+1} = \Pi_ \Theta \left( \theta_ t - \eta_ t g  \right)$  
> **end for**  
> **Output**: $\\{ \theta_t \\}_ {t=1}^T$,
___

where
- $\theta_ 1 \in \mathbb{R}^p$ is the initial cost vector.
- (A-)FOP is the (approximated) FOP oracle.
- $\kappa$ is a nonnegative regularization parameter.
- $\nabla\mathcal{R}$ is the gradient (or a subgradient) of the regularization function.
- $\hat{\theta} \in \mathbb{R}^p$ is an a priory belief or estimate of the true cost vector.
- $\\{ \eta_t \\}_{t=1}^T$ is the step-size sequence.
- $\phi: \mathbb{S} \times \mathbb{X} \to \mathbb{R}^p$ is the feature mapping, which maps a signal-response pair $(\hat{s},x)$ to a feature vector $\phi(\hat{s},x)$.
- $\Theta$ is the set used to encode any prior information or assumption we may have on the expert's true cost function, e.g., nonnegativity of the cost vector.
- $\Pi_\Theta(\theta)$ is the euclidean projection of $\theta$ onto $\Theta$.

When the regularization parameter $\kappa > 0$, $\Theta = \\{\theta \in \mathbb{R}^p : \theta \geq 0 \\}$ and $\mathcal{R}(\theta) = \rVert \theta \rVert_ 1$, the empirical risk minimization problem can be reformulated as

$$
\min_ {\theta \in \Delta_{\kappa}} \ \frac{1}{N}\sum_{i=1}^N \ell_\theta (\hat{s}_i, \hat{x}_i),
$$

where $\Delta_{\kappa} = \\{\theta \in \mathbb{R}^{p} : \kappa \rVert \theta \rVert_ 1 \leq 1, \theta \geq 0\\}$ and it also possible to use an *exponentiated* first-order algorithm.
___
**Exponentiated first-order algorithm**
> **for** $t=1, \ldots, T$ **do**:  
> $\quad$ Sample a batch of examples  $B \subset \\{1, \ldots, N\\}$  
> $\quad$ Compute $x_ j = \text{A-FOP}(\theta_ t, \hat{s}_ j, \hat{x}_ j)$ or $x_ j = \text{FOP}(\theta_ t, \hat{s}_ j)$ for all $j \in B$  
> $\quad$ Compute $g = \sum_ {j \in B} \big( \phi(\hat{s}_ j,\hat{x}_ j) - \phi(\hat{s}_ j,x_ j) \big)$  
> $\quad$ Update step:  
>
>$$
\theta_ {t+1} =
\begin{cases}
\theta_ t \odot \exp(-\eta_ t g) \quad & \text{if} \quad \kappa \rVert \theta_ t \odot \exp(-\eta_t g) \rVert_ 1 \leq 1 \\
\frac{\theta_ t \odot \exp(-\eta_ t g)}{\kappa \rVert \theta_ t \odot \exp(-\eta_ t g) \rVert_ 1} \quad & \text{otherwise}
\end{cases}
$$
>
> **end for**  
> **Output**: $\\{ \theta_ t \\}_ {t=1}^T$,
___
where $\odot$ is element-wise multiplication and $\text{exp}$ is element-wise exponentiation.

**Note**: for the (A-)FOP oracles, ties in the argmin can be broken arbitrarily. Moreover, approximate oracles can also be used. For more details on the derivation of these algorithms and loss functions, see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/2305.07730). For more details on the algorithms implemented in the InvOpt package and their possible variants, please see the documentation of the `FOM` function. For instance, there are different options on how to choose the batch of examples $B$, different options on what value exactly the algorithm returns, e.g., last iterate or (weighted) average, or the option to normalize the subgradients.

## Example 1: binary linear program

In the file `first_order_methods.py`, you will find an example usage of the `FOM` function. For this example, the dataset $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$ is generated by solving the binary linear program

$$
\begin{aligned}
\hat{x}_ i \in \arg\min_ {x} \quad &  \langle \theta,x \rangle \\
\text{s.t.} \quad & \hat{A}_ i x \leq \hat{b}_ i \\
& x \in \\{0,1\\}^n,
\end{aligned}
$$

where $\hat{s}_ i = (\hat{A}_ i, \hat{b}_ i)$ and $\phi(\hat{s},x)=x$. The IO problem is solved using different versions of the first-order algorithm and its exponentiated variant. For a detailed description of this example, please see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/2305.07730).

## Example 2: Vehicle Routing Problem with Time Windows (VRPTW)

In the file `vrptw.py`, you will find an example usage of the `FOM` function for a VRPTW example. You can find the details of this example (i.e., modelling, datasets, algorithm, etc) in [this paper](https://arxiv.org/abs/2307.07357).

## Example 3: Amazon Last Mile Routing Research Challenge

In this [repository](https://github.com/pedroszattoni/amazon-challenge) and [accompanying paper](https://arxiv.org/abs/2307.07357), you will find an example usage of the `FOM` function for the Amazon Last Mile Routing Research Challenge.