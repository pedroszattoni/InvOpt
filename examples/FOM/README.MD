# `FOM`

## Use cases

This function is can be for general FOPs of the form

$$
\min_ {x \in \mathbb{X}(\hat{s})} \ \theta^\top \phi(\hat{s},x).
$$

## Solution method

Given the a dataset of signal-response data $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$,  the `FOM` function solves the IO problem by minimizing the *(Augmented) Suboptimality loss*. To minimize the Augmented Suboptimality loss, the first-order algorithm uses the augmented FOP oracle

$$
\text{A-FOP}(\theta, \hat{s}, \hat{x}) = \arg\min_ {x \in \mathbb{X}(\hat{s})} \big\\{\theta^\top \phi(\hat{s},x) - d(\hat{x},x) \big\\}.
$$

When minimizing the the Suboptimality loss, the algorithm uses a FOP oracle

$$
\text{FOP}(\theta, \hat{s}) = \arg\min_ {x \in \mathbb{X}(\hat{s})} \theta^\top \phi(\hat{s},x).
$$
___
**First-order algorithm**
> **for** $t=1, \ldots, T$ **do**:  
> $\quad$ Sample a batch of examples  $B \subset \\{1, \ldots, N\\}$  
> $\quad$ Compute $x_ j = \text{A-FOP}(\theta_ t, \hat{s}_ j, \hat{x}_ j)$ or $x_ j = \text{FOP}(\theta_ t, \hat{s}_ j)$ for all $j \in B$  
> $\quad$ Compute $g = \sum_ {j \in B} \left( \kappa \nabla\mathcal{R}(\theta_ t - \hat{\theta}) + \phi(\hat{s}_ j,\hat{x}_ j) - \phi(\hat{s}_ j,x_ j) \right)$  
> $\quad$ Update step: $\theta_ {t+1} = \Pi_ \Theta \left( \theta_ t - \eta_ t g  \right)$  
> **end for**  
> **Output**: $\\{ \theta_t \\}_ {t=1}^T$,
___

where
- $\theta_ 1$ is the initial cost vector.
- (A-)FOP is the (approximated) FOP oracle.
- $\kappa$: nonnegative regularization parameter. It can be interpreted as a trade-off parameter between fitting the training dataset (small $\kappa$) and generalization (large $\kappa$).
- $\nabla\mathcal{R}$ is the gradient (or a subgradient) of the regularization function.
- $\hat{\theta}$ is an a priory belief or estimate of the true cost vector.
- $\\{ \eta_t \\}_{t=1}^T$ is the step-size sequence.
- $\phi: \mathbb{S} \times \mathbb{X} \to \mathbb{R}^p$ is the feature mapping, which maps a signal-response pair $(s,x)$ to a feature vector $\phi(s,x)$.
- $\Theta$ is the set used to encode any prior information or assumption we may have on the expert's true cost function, e.g., nonnegativity of the cost vector.
- $\Pi_\Theta(\theta)$ is the euclidean projection of $\theta$ onto $\Theta$.

When the regularization parameter $\kappa > 0$ and $\mathcal{R}(\theta) = \rVert \theta \rVert_ 1$, it also possible to use an *exponentiated* first-order algorithm.
___
**Exponentiated first-order algorithm**
> **for** $t=1, \ldots, T$ **do**:  
> $\quad$ Sample a batch of examples  $B \subset \\{1, \ldots, N\\}$  
> $\quad$ Compute $x_ j = \text{A-FOP}(\theta_ t, \hat{s}_ j, \hat{x}_ j)$ or $x_ j = \text{FOP}(\theta_ t, \hat{s}_ j)$ for all $j \in B$  
> $\quad$ Compute $g = \sum_ {j \in B} \left( \kappa \nabla\mathcal{R}(\theta_ t - \hat{\theta}) + \phi(\hat{s}_ j,\hat{x}_ j) - \phi(\hat{s}_ j,x_ j) \right)$  
> $\quad$ Update step:  
>
>$$  
\theta_ {t+1} = \begin{cases}  
\theta_ t \odot \exp(-\eta_ t g) & \text{if} \quad \kappa \rVert \theta_ t \odot \exp(-\eta_t g) \rVert_ 1 \leq 1 \\  
\frac{\theta_ t \odot \exp(\eta_ t g)}{\kappa \rVert \theta_ t \odot \exp(-\eta_ t g) \rVert_ 1} & \text{otherwise}  
\end{cases}  
$$  
> **end for**  
> **Output**: $\\{ \theta_ t \\}_ {t=1}^T$,
___
where $\odot$ means element-wise multiplication.

>$$\theta_ {t+1} = \begin{cases} \theta_ t \odot \exp(-\eta_ t g) & \text{if} \quad \kappa \rVert \theta_ t \odot \exp(-\eta_t g) \rVert_ 1 \leq 1 \\ \frac{\theta_ t \odot \exp(\eta_ t g)}{\kappa \rVert \theta_ t \odot \exp(-\eta_ t g) \rVert_ 1} & \text{otherwise} \end{cases}$$ 

$$\theta_ {t+1} = \begin{cases} \theta_ t \odot \exp(-\eta_ t g) & \text{if} \quad \kappa \rVert \theta_ t \odot \exp(-\eta_t g) \rVert_ 1 \leq 1 \\ \frac{\theta_ t \odot \exp(\eta_ t g)}{\kappa \rVert \theta_ t \odot \exp(-\eta_ t g) \rVert_ 1} & \text{otherwise} \end{cases}$$ 

**Note**: for the (A-)FOP oracles, ties in the argmin can be broken arbitrarily. Moreover, approximate oracles can also be used. For more details on the algorithms implement in the InvOpt package, please see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/0000.00000) and the documentation of the `FOM` function.

## Example: binary linear program

In the file `first_order_methods.py`, you will find an example usage of the `FOM` function. For this example, the dataset $\mathcal{D} = \\{(\hat{s}_ i, \hat{x}_ i)\\}_ {i=1}^N$ is generated by solving the binary linear program

$$
\begin{aligned}
\hat{x}_ i \in \arg\min_ {x} \quad &  \theta^\top x \\
\text{s.t.} \quad & A_ i x \leq b_ i \\
& x \in \\{0,1\\}^n,
\end{aligned}
$$

where $\hat{s}_ i = (A_ i, b_ i)$ and the IO problem is solved using different versions fo the first-order algorithm and its exponentiated variant. For a detailed description of this example, please see [Zattoni Scroccaro et al. (2023)](https://arxiv.org/abs/0000.00000).